{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir tmp\n",
    "! mkdir tmp/model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path):\n",
    "    with open(path) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    albums_uri = []\n",
    "    tracks_uri = []\n",
    "    artists_uri = []\n",
    "    position = []\n",
    "    almbums_name = []\n",
    "    artists_name = []\n",
    "    tracks_name = []\n",
    "    playlist_ids = []\n",
    "    playlist_name = []\n",
    "    labels = []\n",
    "\n",
    "    playlist_id = 0\n",
    "\n",
    "    for playlist in data['playlists']:\n",
    "        playlist_id += 1\n",
    "        for track in playlist['tracks']:\n",
    "            playlist_name.append(playlist['name'])\n",
    "            playlist_ids.append(playlist_id)\n",
    "            albums_uri.append(track['album_uri'])\n",
    "            tracks_uri.append(track['track_uri'])\n",
    "            artists_uri.append(track['artist_uri'])\n",
    "            position.append(int(track['pos']))\n",
    "            almbums_name.append(track['album_name'])\n",
    "            artists_name.append(track['artist_name'])\n",
    "            tracks_name.append(track['track_name'])\n",
    "            labels.append(1.)\n",
    "        \n",
    "    d = {\n",
    "        'playlist_ids': playlist_ids,\n",
    "        'playlist_name': playlist_name,\n",
    "        'albums_uri': albums_uri,\n",
    "        'tracks_uri': tracks_uri,\n",
    "        'artists_uri': artists_uri,\n",
    "        'position': position,\n",
    "        'albums_name': almbums_name,\n",
    "        'artists_name': artists_name,\n",
    "        'tracks_name': tracks_name,\n",
    "        'labels': labels\n",
    "    }\n",
    "    df = pd.DataFrame(data=d)\n",
    "    return df\n",
    "\n",
    "df = load_dataset('./dataset/data.json')\n",
    "df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train / test samples\n",
    "def split_train_test(df, ratio=.2):\n",
    "    df['test'] = 0\n",
    "    for playlist in df['playlist_ids'].unique():\n",
    "        playlist_tracks = df[ df['playlist_ids'] == playlist ]['tracks_uri'].tolist()\n",
    "        n = len(playlist_tracks)\n",
    "        np.random.shuffle(playlist_tracks)\n",
    "        test_tracks = playlist_tracks[:int(n*ratio)]\n",
    "        df.loc[(df['tracks_uri'].isin(test_tracks)) & (df['playlist_ids'] == playlist), 'test'] = 1\n",
    "    \n",
    "    return df[ df['test'] == 0 ], df[ df['test'] == 1 ]\n",
    "\n",
    "train_df, test_df = split_train_test(df)\n",
    "\n",
    "print('Train shape:', train_df.shape)\n",
    "print('Test shape:', test_df.shape)\n",
    "print('All data shape:', df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative sampling\n",
    "def sample_negative_data(df):\n",
    "    N = df['playlist_ids'].nunique()\n",
    "    all_tracks = df[['albums_name', 'albums_uri', 'artists_name', 'artists_uri', 'tracks_name', 'tracks_uri']].drop_duplicates()\n",
    "    tracks_dict = all_tracks.to_dict('index')\n",
    "    tracks_uri_to_id = {v['tracks_uri']: k for k, v in tracks_dict.items()}\n",
    "\n",
    "    ids = []\n",
    "    albums_uri = []\n",
    "    tracks_uri = []\n",
    "    artists_uri = []\n",
    "    position = []\n",
    "    almbums_name = []\n",
    "    artists_name = []\n",
    "    tracks_name = []\n",
    "    playlist_ids = []\n",
    "    playlist_name = []\n",
    "    labels = []\n",
    "\n",
    "    for i, playlist_id in enumerate(df['playlist_ids'].unique()):\n",
    "        if i % 100 == 0:\n",
    "            print('Sampling negative data... processing playlist {}/{}'.format(i+1, N))\n",
    "\n",
    "        name = df[ df['playlist_ids'] == playlist_id ].iloc[0]['playlist_name']\n",
    "        positive_tracks = df[ df['playlist_ids'] == playlist_id ]['tracks_uri'].tolist()\n",
    "        negative_tracks = all_tracks['tracks_uri'].unique().tolist()\n",
    "        negative_tracks = np.setdiff1d(negative_tracks, positive_tracks)\n",
    "        np.random.shuffle(negative_tracks)\n",
    "        negative_tracks = negative_tracks[:len(positive_tracks)]\n",
    "\n",
    "        for track in negative_tracks:\n",
    "            track_idx = tracks_uri_to_id[track]\n",
    "            track_data = tracks_dict[track_idx]\n",
    "        \n",
    "            playlist_name.append(name)\n",
    "            playlist_ids.append(playlist_id)\n",
    "            albums_uri.append(track_data['albums_uri'])\n",
    "            tracks_uri.append(track_data['tracks_uri'])\n",
    "            artists_uri.append(track_data['artists_uri'])\n",
    "            position.append(-1)\n",
    "            almbums_name.append(track_data['albums_name'])\n",
    "            artists_name.append(track_data['artists_name'])\n",
    "            tracks_name.append(track_data['tracks_name'])\n",
    "            labels.append(0.)\n",
    "        \n",
    "    d = {\n",
    "        'playlist_ids': playlist_ids,\n",
    "        'playlist_name': playlist_name,\n",
    "        'albums_uri': albums_uri,\n",
    "        'tracks_uri': tracks_uri,\n",
    "        'artists_uri': artists_uri,\n",
    "        'position': position,\n",
    "        'albums_name': almbums_name,\n",
    "        'artists_name': artists_name,\n",
    "        'tracks_name': tracks_name,\n",
    "        'labels': labels\n",
    "    }\n",
    "    neg_df = pd.DataFrame(data=d)\n",
    "    df = pd.concat([df, neg_df])\n",
    "    \n",
    "    return df\n",
    "\n",
    "train_df = sample_negative_data(train_df)\n",
    "train_df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define train / test split\n",
    "ratio = .2\n",
    "df['sample'] = np.random.rand(df.shape[0])\n",
    "df['test'] = np.where(np.random.rand(df.shape[0]) < ratio, 1, 0)\n",
    "df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_playlist = df['playlist_ids'].nunique()\n",
    "n_artist= df['artists_uri'].nunique()\n",
    "n_albumns = df['albums_uri'].nunique()\n",
    "n_tracks = df['tracks_uri'].nunique()\n",
    "\n",
    "print('N Playlists:', n_playlist)\n",
    "print('N Artists:', n_artist)\n",
    "print('N Albums:', n_albumns)\n",
    "print('N Tracks:', n_albumns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Max values to -1 pad arrays\n",
    "tmp = train_df.groupby(['playlist_ids'])[['tracks_uri', 'albums_uri', 'artists_uri']].nunique()\n",
    "max_tracks = tmp['tracks_uri'].max()\n",
    "max_albums = tmp['albums_uri'].max()\n",
    "max_artists = tmp['artists_uri'].max()\n",
    "del tmp\n",
    "\n",
    "print('Max Tracks:', max_tracks)\n",
    "print('Max Albums:', max_albums)\n",
    "print('Max Artists:', max_artists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Entities maps\n",
    "album_to_int = {entity: i for i, entity in enumerate(df['albums_uri'].unique())}\n",
    "artist_to_int = {entity: i for i, entity in enumerate(df['artists_uri'].unique())}\n",
    "track_to_int = {entity: i for i, entity in enumerate(df['tracks_uri'].unique())}\n",
    "playlist_to_int = {entity: i for i, entity in enumerate(df['playlist_ids'].unique())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Prepare dataset\n",
    "def prepare_dataset(df):\n",
    "    labels = []\n",
    "    playlist_tracks = []\n",
    "    playlist_artists = []\n",
    "    playlist_albums = []\n",
    "\n",
    "    target_playlists = []\n",
    "    target_artists = []\n",
    "    target_tracks = []\n",
    "    target_albums = []\n",
    "\n",
    "    for playlist in df['playlist_ids'].unique():\n",
    "        n = df[ df['playlist_ids'] == playlist ].shape[0]\n",
    "\n",
    "        track_ids = df[ df['playlist_ids'] == playlist ]['tracks_uri'].map(lambda x: track_to_int[x]).tolist()\n",
    "        target_tracks.extend(track_ids)\n",
    "        track_ids = np.unique(track_ids)\n",
    "        track_ids = np.pad(track_ids, (0, max_tracks-len(track_ids)), 'constant', constant_values=-1)\n",
    "        [playlist_tracks.append(track_ids) for _ in range(n)]\n",
    "    \n",
    "        artist_ids = df[ df['playlist_ids'] == playlist ]['artists_uri'].map(lambda x: artist_to_int[x]).tolist()\n",
    "        target_artists.extend(artist_ids)\n",
    "        artist_ids = np.unique(artist_ids)\n",
    "        artist_ids = np.pad(artist_ids, (0, max_artists-len(artist_ids)), 'constant', constant_values=-1)\n",
    "        [playlist_artists.append(artist_ids) for _ in range(n)]\n",
    "    \n",
    "        albums_ids = df[ df['playlist_ids'] == playlist ]['albums_uri'].map(lambda x: album_to_int[x]).tolist()\n",
    "        target_albums.extend(albums_ids)\n",
    "        albums_ids = np.unique(albums_ids)\n",
    "        albums_ids = np.pad(albums_ids, (0, max_albums-len(albums_ids)), 'constant', constant_values=-1)\n",
    "        [playlist_albums.append(albums_ids) for _ in range(n)]\n",
    "    \n",
    "        track_lavels = df[ df['playlist_ids'] == playlist ]['labels'].tolist()\n",
    "        labels.extend(track_lavels)\n",
    "    \n",
    "        playlist_id = playlist_to_int[playlist]\n",
    "        [target_playlists.append(playlist_id) for _ in range(n)]\n",
    "    \n",
    "    playlist_tracks = np.array(playlist_tracks)\n",
    "    playlist_artists = np.array(playlist_artists)\n",
    "    playlist_albums = np.array(playlist_albums)\n",
    "    \n",
    "    return {\n",
    "        'labels': labels,\n",
    "        'playlist_tracks': playlist_tracks,\n",
    "        'playlist_artists': playlist_artists,\n",
    "        'playlist_albums': playlist_albums,\n",
    "        'target_playlists': target_playlists,\n",
    "        'target_artists': target_artists,\n",
    "        'target_tracks': target_tracks,\n",
    "        'target_albums': target_albums\n",
    "    }\n",
    "\n",
    "data = prepare_dataset(train_df)\n",
    "labels = data['labels']\n",
    "playlist_tracks = data['playlist_tracks']\n",
    "playlist_artists = data['playlist_artists']\n",
    "playlist_albums = data['playlist_albums']\n",
    "target_playlists = data['target_playlists']\n",
    "target_artists = data['target_artists']\n",
    "target_tracks = data['target_tracks']\n",
    "target_albums = data['target_albums']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Len playlist_tracks:', len(playlist_tracks))\n",
    "print('Len playlist_artists:', len(playlist_artists))\n",
    "print('Len playlist_albums:', len(playlist_albums))\n",
    "\n",
    "print('Len target_playlists:', len(target_playlists))\n",
    "print('Len target_artists:', len(target_artists))\n",
    "print('Len target_tracks:', len(target_tracks))\n",
    "print('Len target_albums:', len(target_albums))\n",
    "\n",
    "print('Len labels:', len(labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo Wide & Deep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K = 32\n",
    "\n",
    "# Define columns input\n",
    "list_track_column = tf.feature_column.categorical_column_with_identity(\n",
    "    key='playlist_tracks',\n",
    "    num_buckets=n_tracks)\n",
    "\n",
    "list_artist_column = tf.feature_column.categorical_column_with_identity(\n",
    "    key='playlist_artists',\n",
    "    num_buckets=n_artist)\n",
    "\n",
    "list_album_column = tf.feature_column.categorical_column_with_identity(\n",
    "    key='playlist_albums',\n",
    "    num_buckets=n_albumns)\n",
    "\n",
    "target_artist_column = tf.feature_column.categorical_column_with_identity(\n",
    "    key='target_artist',\n",
    "    num_buckets=n_artist)\n",
    "\n",
    "target_playlist_column = tf.feature_column.categorical_column_with_identity(\n",
    "    key='target_playlist',\n",
    "    num_buckets=n_playlist)\n",
    "\n",
    "target_track_column = tf.feature_column.categorical_column_with_identity(\n",
    "    key='target_track',\n",
    "    num_buckets=n_tracks)\n",
    "\n",
    "target_album_column = tf.feature_column.categorical_column_with_identity(\n",
    "    key='target_album',\n",
    "    num_buckets=n_albumns)\n",
    "\n",
    "# Wide columns\n",
    "wide_columns = [\n",
    "    tf.feature_column.indicator_column(list_track_column),\n",
    "    tf.feature_column.indicator_column(list_artist_column),\n",
    "    tf.feature_column.indicator_column(list_album_column)\n",
    "]\n",
    "\n",
    "deep_columns = [\n",
    "    tf.feature_column.embedding_column(\n",
    "        categorical_column=target_artist_column,\n",
    "        dimension=K),\n",
    "    tf.feature_column.embedding_column(\n",
    "        categorical_column=target_playlist_column,\n",
    "        dimension=K),\n",
    "    tf.feature_column.embedding_column(\n",
    "        categorical_column=target_track_column,\n",
    "        dimension=K),\n",
    "    tf.feature_column.embedding_column(\n",
    "        categorical_column=target_album_column,\n",
    "        dimension=K)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dataset API to consume data\n",
    "def input_fn(batch_size, sample=None):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(({\n",
    "        'playlist_tracks': playlist_tracks,\n",
    "        'playlist_artists': playlist_artists,\n",
    "        'playlist_albums': playlist_albums,\n",
    "        'target_artist': target_artists,\n",
    "        'target_playlist': target_playlists,\n",
    "        'target_track': target_tracks,\n",
    "        'target_album': target_albums}, labels))\n",
    "    \n",
    "    if sample is not None:\n",
    "        print('Subset')\n",
    "        dataset = dataset.shuffle(buffer_size=sample)\n",
    "    else:\n",
    "        sample = len(labels)\n",
    "        dataset = dataset.shuffle(buffer_size=sample)\n",
    "\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "batch_size = 126\n",
    "\n",
    "model = tf.estimator.DNNLinearCombinedRegressor(\n",
    "        model_dir='tmp/model',\n",
    "        linear_feature_columns=wide_columns,\n",
    "        dnn_feature_columns=deep_columns,\n",
    "        dnn_hidden_units=[100, 50])\n",
    "\n",
    "for e in range(num_epochs):\n",
    "    model.train(input_fn=lambda: input_fn(batch_size))\n",
    "    print('Done epoch', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dcg(y, y_hat):\n",
    "    rel = np.array([float(np.isin(i, y_hat)) for i in y])\n",
    "    metric = np.sum(rel / np.log2(np.arange(2, rel.size + 2)))\n",
    "    return metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load model\n",
    "model = tf.estimator.DNNLinearCombinedRegressor(\n",
    "        model_dir='tmp/model',\n",
    "        linear_feature_columns=wide_columns,\n",
    "        dnn_feature_columns=deep_columns,\n",
    "        dnn_hidden_units=[100, 50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate target data\n",
    "def predict(model, df, playlist_id, ground_truth, top_n=10):\n",
    "    df = df[['albums_name', 'albums_uri', 'artists_name', 'artists_uri', 'tracks_name', 'tracks_uri']].drop_duplicates()\n",
    "    n = ground_truth.shape[0]\n",
    "    df = df.sample(3 * n)\n",
    "\n",
    "    df['playlist_ids'] = playlist_id\n",
    "    df = pd.concat([df, ground_truth])\n",
    "    data = prepare_dataset(df)\n",
    "\n",
    "    labels = data['labels']\n",
    "    playlist_tracks = data['playlist_tracks']\n",
    "    playlist_artists = data['playlist_artists']\n",
    "    playlist_albums = data['playlist_albums']\n",
    "    target_playlists = data['target_playlists']\n",
    "    target_artists = data['target_artists']\n",
    "    target_tracks = data['target_tracks']\n",
    "    target_albums = data['target_albums']\n",
    "    \n",
    "    batch_size = 64\n",
    "    results = model.predict(input_fn=lambda: input_fn(batch_size))\n",
    "    predictions = []\n",
    "\n",
    "    for track, result in zip(target_tracks, results):\n",
    "        predictions.append((track, result['predictions'][0]))\n",
    "    \n",
    "    \n",
    "    ground_truth = ground_truth['tracks_uri'].map(lambda x: track_to_int[x]).tolist()\n",
    "    predictions = sorted(predictions, key=lambda x: x[1], reverse=True)\n",
    "    predictions = list(map(lambda x: x[0], predictions))[:top_n]\n",
    "        \n",
    "    return predictions, ground_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Test model\n",
    "max_eval = 10\n",
    "metric = 0.\n",
    "N = test_df['playlist_ids'].nunique()\n",
    "print_every_n = 2\n",
    "\n",
    "for i, playlist_id in enumerate(test_df['playlist_ids'].unique()[:max_eval]):\n",
    "    if i % print_every_n == 0:\n",
    "        print('Processing playlist {} / {}'.format(i+1, max_eval))\n",
    "        print('Mean DCG:', metric / N)\n",
    "    ground_truth = test_df[ test_df['playlist_ids'] == playlist_id ]\n",
    "    y_hat, y = predict(model, df, playlist_id, ground_truth)\n",
    "    metric += dcg(y, y_hat)\n",
    "    \n",
    "metric /= N\n",
    "    \n",
    "print('Mean DCG:', metric)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recsys-class",
   "language": "python",
   "name": "recsys-class"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
