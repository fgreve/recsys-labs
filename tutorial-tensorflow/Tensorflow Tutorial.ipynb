{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial Tensorflow\n",
    "\n",
    "En este tutorial se implementará el modelo de [factorización matricial](http://base.sjtu.edu.cn/~bjshen/2.pdf) con la librería [tensorflow](https://www.tensorflow.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instalación de dependencias\n",
    "```\n",
    "$ pip install numpy\n",
    "$ pip install pandas\n",
    "$ pip install tensorflow\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo primero de todo es importar las librerías que se utilizarán en este tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: tmp: File exists\r\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import time\n",
    "! mkdir tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importación de datos\n",
    "\n",
    "En este tutorial se utilizará el mismo set de datos que el laboratorio práctico Nº2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Users: 6040\n",
      "# Items: 3706\n",
      "# Data: 1000209\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1193</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>661</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>914</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3408</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2355</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  movie_id  rating\n",
       "0        1      1193       5\n",
       "1        1       661       3\n",
       "2        1       914       3\n",
       "3        1      3408       4\n",
       "4        1      2355       5"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset\n",
    "data_path = '../assignment-2/dataset/ratings.dat'\n",
    "headers = ['user_id', 'movie_id', 'rating', 'timestamp']\n",
    "data = pd.read_csv(data_path,\n",
    "                   names=headers,\n",
    "                   delimiter=';',\n",
    "                   usecols=['user_id', 'movie_id', 'rating'])\n",
    "\n",
    "print('# Users:', data['user_id'].nunique())\n",
    "print('# Items:', data['movie_id'].nunique())\n",
    "print('# Data:', data.shape[0])\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez cargado los datos transformamos los ID de usuario y películas mediante un diccionario para que tengan orden correlativo desde 0 a N.\n",
    "\n",
    "Luego separamos el set de entrenamiento y de test, dejando el 20% de los datos para este último.\n",
    "\n",
    "También implementamos una función que nos entregará los datos agrupados en batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepate batches\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Transform data to int\n",
    "user_to_int = {user: i for i, user in enumerate(data['user_id'].unique())}\n",
    "item_to_int = {item: i for i, item in enumerate(data['movie_id'].unique())}\n",
    "\n",
    "# Map user and items to int\n",
    "user_data = data['user_id'].map(lambda user: user_to_int[user]).tolist()\n",
    "item_data = data['movie_id'].map(lambda item: item_to_int[item]).tolist()\n",
    "rating_data = data['rating'].tolist()\n",
    "\n",
    "# split into train / test\n",
    "u_train, u_test, v_train, v_test, r_train, r_test = train_test_split(\n",
    "    user_data, item_data, rating_data, test_size=0.2)\n",
    "\n",
    "def get_batch(user_data, item_data, rating_data, batch_size=32):\n",
    "    # Generate complete batches\n",
    "    count = 0\n",
    "    max_len = len(rating_data)\n",
    "    n_batches = max_len // batch_size\n",
    "    \n",
    "    # Shuffle data\n",
    "    user_data, item_data, rating_data = shuffle(user_data, item_data, rating_data)\n",
    "    \n",
    "    user_data = user_data[0:n_batches*batch_size]\n",
    "    item_data = item_data[0:n_batches*batch_size]\n",
    "    rating_data = rating_data[0:n_batches*batch_size]\n",
    "    \n",
    "    for i in range(0, max_len, batch_size):\n",
    "        count += 1\n",
    "        u = user_data[i:i+batch_size]\n",
    "        v = item_data[i:i+batch_size]\n",
    "        y = rating_data[i:i+batch_size]\n",
    "            \n",
    "        yield u, v, y, count, n_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición del grafo de cómputo\n",
    "\n",
    "En la siguiente celda se utiliza el framework para generar el grafo de cómputo\n",
    "\n",
    "- El primer paso define los parámetros del grafo los cuales corresponden a la cantidad de usuarios e items del set de datos, como también el hiperparámetro k, alpha y el _learning rate_\n",
    "- Luego se definen las variables que serán alimentadas desde el diccionario.\n",
    "- Se inicializan las matrices P y Q del modelo con una distribución uniforme entre [-1, 1]\n",
    "- Los _embeddings_ de usuarios e item son obtenidos mediante una tabla _lookup_\n",
    "- *y_hat* corresponde a la predicción del modelo, i.e. al producto vectorial entre los embeddings\n",
    "- Finalmente definimos la pérdida que se desea optimizar y se crea el objeto optimizador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the graph model\n",
    "\n",
    "# 1. Define model parameters\n",
    "n_users = data['user_id'].nunique()\n",
    "n_items = data['movie_id'].nunique()\n",
    "k = 40\n",
    "alpha = tf.constant(.001, name='alpha')\n",
    "learning_rate = .01\n",
    "\n",
    "# 2. Define variables that are fed through the dictionary session\n",
    "# User, item and ratings placeholders\n",
    "user_input = tf.placeholder(tf.int32, [None], name='user_input')\n",
    "item_input = tf.placeholder(tf.int32, [None], name='item_input')\n",
    "y = tf.placeholder(tf.float32, [None], name='ratings_input')\n",
    "\n",
    "# 3. Define and Initilize matrix embeddings\n",
    "# User embeddings come from P matrix\n",
    "# Item embeddings come from Q matrix\n",
    "\n",
    "with tf.name_scope('P_matrix'):\n",
    "    P_matrix = tf.Variable(tf.random_uniform((n_users, k), -1, 1), name='user_embeddings')\n",
    "    tf.summary.tensor_summary('P_matrix', P_matrix)\n",
    "with tf.name_scope('Q_matrix'):\n",
    "    Q_matrix = tf.Variable(tf.random_uniform((n_items, k), -1, 1), name='item_embeddings')\n",
    "    tf.summary.tensor_summary('Q_matrix', Q_matrix)\n",
    "\n",
    "# 4. Fetch embeddings with a lookup table\n",
    "# Define user and item embedding\n",
    "user_embed = tf.nn.embedding_lookup(P_matrix, user_input, name='user_embed')\n",
    "item_embed = tf.nn.embedding_lookup(Q_matrix, item_input, name='item_embed')\n",
    "\n",
    "# 5. Compute prediction\n",
    "with tf.name_scope('prediction'):\n",
    "    y_hat = tf.reduce_sum(tf.multiply(user_embed, item_embed), 1)\n",
    "    tf.summary.scalar('prediction', y_hat)\n",
    "    pred_histogram = tf.summary.histogram(\"mean_prediction\", y_hat)\n",
    "\n",
    "    \n",
    "# Compute loss function\n",
    "# loss = 1/n (y - y_hat) ** 2\n",
    "mse_loss = tf.losses.mean_squared_error(y, y_hat)\n",
    "\n",
    "# reg_loss = alpha * (||p|| + ||q||)\n",
    "reg_loss = tf.add(tf.multiply(alpha, tf.nn.l2_loss(user_embed)), tf.multiply(alpha, tf.nn.l2_loss(item_embed)))\n",
    "\n",
    "with tf.name_scope('error'):\n",
    "    loss = tf.add(mse_loss, reg_loss)\n",
    "    tf.summary.scalar('error', loss)\n",
    "    loss_histogram = tf.summary.histogram(\"mean_loss\", loss)\n",
    "\n",
    "    \n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento del modelo\n",
    "\n",
    "El primer paso para poder ejecutar el modelo ya definido es generar un objeto de sesión e inicializar las variables. Luego iteramos las veces que se quiere para ajustar el modelo.\n",
    "\n",
    "Para el ajuste del modelo, se genera un diccionario que alimenta el grafo de cómputo y luego se evalúa la sesión con el diccionario. En este caso se evalúa la pérdida y el objeto optimizador, el cual es el encargado de calcular el gradiente y actualizar los parámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch: 1/20   Batch: 100/781 (12.80%)   Train loss: 24.7124023438   0.0143 sec/batch\n",
      "[Train] Epoch: 1/20   Batch: 200/781 (25.61%)   Train loss: 20.5979576111   0.0098 sec/batch\n",
      "[Train] Epoch: 1/20   Batch: 300/781 (38.41%)   Train loss: 16.9126205444   0.0081 sec/batch\n",
      "[Train] Epoch: 1/20   Batch: 400/781 (51.22%)   Train loss: 10.9266796112   0.0076 sec/batch\n",
      "[Train] Epoch: 1/20   Batch: 500/781 (64.02%)   Train loss: 7.2323837280   0.0072 sec/batch\n",
      "[Train] Epoch: 1/20   Batch: 600/781 (76.82%)   Train loss: 6.2153320312   0.0070 sec/batch\n",
      "[Train] Epoch: 1/20   Batch: 700/781 (89.63%)   Train loss: 5.6692728996   0.0070 sec/batch\n",
      "[Validation] Epoch: 1/20   Validation loss: 5.2253034347   0.6958 sec\n",
      "[Train] Epoch: 2/20   Batch: 100/781 (12.80%)   Train loss: 4.9673781395   0.0799 sec/batch\n",
      "[Train] Epoch: 2/20   Batch: 200/781 (25.61%)   Train loss: 4.7676677704   0.0427 sec/batch\n",
      "[Train] Epoch: 2/20   Batch: 300/781 (38.41%)   Train loss: 4.6218252182   0.0302 sec/batch\n",
      "[Train] Epoch: 2/20   Batch: 400/781 (51.22%)   Train loss: 4.6091890335   0.0244 sec/batch\n",
      "[Train] Epoch: 2/20   Batch: 500/781 (64.02%)   Train loss: 4.5275263786   0.0206 sec/batch\n",
      "[Train] Epoch: 2/20   Batch: 600/781 (76.82%)   Train loss: 4.4877748489   0.0183 sec/batch\n",
      "[Train] Epoch: 2/20   Batch: 700/781 (89.63%)   Train loss: 4.3758711815   0.0165 sec/batch\n",
      "[Validation] Epoch: 2/20   Validation loss: 4.4453224940   0.5141 sec\n",
      "[Train] Epoch: 3/20   Batch: 100/781 (12.80%)   Train loss: 4.3951783180   0.1411 sec/batch\n",
      "[Train] Epoch: 3/20   Batch: 200/781 (25.61%)   Train loss: 4.3429508209   0.0741 sec/batch\n",
      "[Train] Epoch: 3/20   Batch: 300/781 (38.41%)   Train loss: 4.3469944000   0.0510 sec/batch\n",
      "[Train] Epoch: 3/20   Batch: 400/781 (51.22%)   Train loss: 4.4297733307   0.0395 sec/batch\n",
      "[Train] Epoch: 3/20   Batch: 500/781 (64.02%)   Train loss: 4.3783302307   0.0325 sec/batch\n",
      "[Train] Epoch: 3/20   Batch: 600/781 (76.82%)   Train loss: 4.5159587860   0.0279 sec/batch\n",
      "[Train] Epoch: 3/20   Batch: 700/781 (89.63%)   Train loss: 4.3530912399   0.0249 sec/batch\n",
      "[Validation] Epoch: 3/20   Validation loss: 4.3974946805   0.4445 sec\n",
      "[Train] Epoch: 4/20   Batch: 100/781 (12.80%)   Train loss: 4.3324618340   0.1991 sec/batch\n",
      "[Train] Epoch: 4/20   Batch: 200/781 (25.61%)   Train loss: 4.4035797119   0.1019 sec/batch\n",
      "[Train] Epoch: 4/20   Batch: 300/781 (38.41%)   Train loss: 4.4154276848   0.0696 sec/batch\n",
      "[Train] Epoch: 4/20   Batch: 400/781 (51.22%)   Train loss: 4.3822612762   0.0541 sec/batch\n",
      "[Train] Epoch: 4/20   Batch: 500/781 (64.02%)   Train loss: 4.3533754349   0.0445 sec/batch\n",
      "[Train] Epoch: 4/20   Batch: 600/781 (76.82%)   Train loss: 4.4240031242   0.0381 sec/batch\n",
      "[Train] Epoch: 4/20   Batch: 700/781 (89.63%)   Train loss: 4.3688592911   0.0335 sec/batch\n",
      "[Validation] Epoch: 4/20   Validation loss: 4.4115321379   0.4201 sec\n",
      "[Train] Epoch: 5/20   Batch: 100/781 (12.80%)   Train loss: 4.3293542862   0.2602 sec/batch\n",
      "[Train] Epoch: 5/20   Batch: 200/781 (25.61%)   Train loss: 4.4031839371   0.1333 sec/batch\n",
      "[Train] Epoch: 5/20   Batch: 300/781 (38.41%)   Train loss: 4.3533349037   0.0912 sec/batch\n",
      "[Train] Epoch: 5/20   Batch: 400/781 (51.22%)   Train loss: 4.4598178864   0.0704 sec/batch\n",
      "[Train] Epoch: 5/20   Batch: 500/781 (64.02%)   Train loss: 4.3766360283   0.0573 sec/batch\n",
      "[Train] Epoch: 5/20   Batch: 600/781 (76.82%)   Train loss: 4.4489479065   0.0488 sec/batch\n",
      "[Train] Epoch: 5/20   Batch: 700/781 (89.63%)   Train loss: 4.4217972755   0.0425 sec/batch\n",
      "[Validation] Epoch: 5/20   Validation loss: 4.4207248052   0.5560 sec\n",
      "[Train] Epoch: 6/20   Batch: 100/781 (12.80%)   Train loss: 4.3599920273   0.3208 sec/batch\n",
      "[Train] Epoch: 6/20   Batch: 200/781 (25.61%)   Train loss: 4.3978900909   0.1631 sec/batch\n",
      "[Train] Epoch: 6/20   Batch: 300/781 (38.41%)   Train loss: 4.4357047081   0.1104 sec/batch\n",
      "[Train] Epoch: 6/20   Batch: 400/781 (51.22%)   Train loss: 4.4959583282   0.0844 sec/batch\n",
      "[Train] Epoch: 6/20   Batch: 500/781 (64.02%)   Train loss: 4.3631343842   0.0686 sec/batch\n",
      "[Train] Epoch: 6/20   Batch: 600/781 (76.82%)   Train loss: 4.4215822220   0.0580 sec/batch\n",
      "[Train] Epoch: 6/20   Batch: 700/781 (89.63%)   Train loss: 4.4465413094   0.0504 sec/batch\n",
      "[Validation] Epoch: 6/20   Validation loss: 4.4199665779   0.4343 sec\n",
      "[Train] Epoch: 7/20   Batch: 100/781 (12.80%)   Train loss: 4.4086484909   0.3764 sec/batch\n",
      "[Train] Epoch: 7/20   Batch: 200/781 (25.61%)   Train loss: 4.3905935287   0.1924 sec/batch\n",
      "[Train] Epoch: 7/20   Batch: 300/781 (38.41%)   Train loss: 4.4869914055   0.1310 sec/batch\n",
      "[Train] Epoch: 7/20   Batch: 400/781 (51.22%)   Train loss: 4.4417214394   0.0998 sec/batch\n",
      "[Train] Epoch: 7/20   Batch: 500/781 (64.02%)   Train loss: 4.4263863564   0.0810 sec/batch\n",
      "[Train] Epoch: 7/20   Batch: 600/781 (76.82%)   Train loss: 4.4697637558   0.0684 sec/batch\n",
      "[Train] Epoch: 7/20   Batch: 700/781 (89.63%)   Train loss: 4.3343124390   0.0594 sec/batch\n",
      "[Validation] Epoch: 7/20   Validation loss: 4.4038241900   0.6952 sec\n",
      "[Train] Epoch: 8/20   Batch: 100/781 (12.80%)   Train loss: 4.4109759331   0.4415 sec/batch\n",
      "[Train] Epoch: 8/20   Batch: 200/781 (25.61%)   Train loss: 4.4701719284   0.2232 sec/batch\n",
      "[Train] Epoch: 8/20   Batch: 300/781 (38.41%)   Train loss: 4.3945627213   0.1504 sec/batch\n",
      "[Train] Epoch: 8/20   Batch: 400/781 (51.22%)   Train loss: 4.4407925606   0.1142 sec/batch\n",
      "[Train] Epoch: 8/20   Batch: 500/781 (64.02%)   Train loss: 4.4155521393   0.0926 sec/batch\n",
      "[Train] Epoch: 8/20   Batch: 600/781 (76.82%)   Train loss: 4.3467497826   0.0780 sec/batch\n",
      "[Train] Epoch: 8/20   Batch: 700/781 (89.63%)   Train loss: 4.4181342125   0.0677 sec/batch\n",
      "[Validation] Epoch: 8/20   Validation loss: 4.3971538006   0.4852 sec\n",
      "[Train] Epoch: 9/20   Batch: 100/781 (12.80%)   Train loss: 4.4985494614   0.4965 sec/batch\n",
      "[Train] Epoch: 9/20   Batch: 200/781 (25.61%)   Train loss: 4.3988523483   0.2507 sec/batch\n",
      "[Train] Epoch: 9/20   Batch: 300/781 (38.41%)   Train loss: 4.3810539246   0.1693 sec/batch\n",
      "[Train] Epoch: 9/20   Batch: 400/781 (51.22%)   Train loss: 4.4006662369   0.1284 sec/batch\n",
      "[Train] Epoch: 9/20   Batch: 500/781 (64.02%)   Train loss: 4.3657894135   0.1041 sec/batch\n",
      "[Train] Epoch: 9/20   Batch: 600/781 (76.82%)   Train loss: 4.4358654022   0.0877 sec/batch\n",
      "[Train] Epoch: 9/20   Batch: 700/781 (89.63%)   Train loss: 4.3776011467   0.0761 sec/batch\n",
      "[Validation] Epoch: 9/20   Validation loss: 4.3891551923   0.5090 sec\n",
      "[Train] Epoch: 10/20   Batch: 100/781 (12.80%)   Train loss: 4.4770822525   0.5579 sec/batch\n",
      "[Train] Epoch: 10/20   Batch: 200/781 (25.61%)   Train loss: 4.3468856812   0.2821 sec/batch\n",
      "[Train] Epoch: 10/20   Batch: 300/781 (38.41%)   Train loss: 4.3837232590   0.1904 sec/batch\n",
      "[Train] Epoch: 10/20   Batch: 400/781 (51.22%)   Train loss: 4.3313689232   0.1447 sec/batch\n",
      "[Train] Epoch: 10/20   Batch: 500/781 (64.02%)   Train loss: 4.3617205620   0.1173 sec/batch\n",
      "[Train] Epoch: 10/20   Batch: 600/781 (76.82%)   Train loss: 4.4166097641   0.0989 sec/batch\n",
      "[Train] Epoch: 10/20   Batch: 700/781 (89.63%)   Train loss: 4.3314714432   0.0858 sec/batch\n",
      "[Validation] Epoch: 10/20   Validation loss: 4.3848950973   0.8229 sec\n",
      "[Train] Epoch: 11/20   Batch: 100/781 (12.80%)   Train loss: 4.3461346626   0.6364 sec/batch\n",
      "[Train] Epoch: 11/20   Batch: 200/781 (25.61%)   Train loss: 4.3920173645   0.3213 sec/batch\n",
      "[Train] Epoch: 11/20   Batch: 300/781 (38.41%)   Train loss: 4.3996343613   0.2159 sec/batch\n",
      "[Train] Epoch: 11/20   Batch: 400/781 (51.22%)   Train loss: 4.3836355209   0.1635 sec/batch\n",
      "[Train] Epoch: 11/20   Batch: 500/781 (64.02%)   Train loss: 4.3298225403   0.1320 sec/batch\n",
      "[Train] Epoch: 11/20   Batch: 600/781 (76.82%)   Train loss: 4.4517107010   0.1110 sec/batch\n",
      "[Train] Epoch: 11/20   Batch: 700/781 (89.63%)   Train loss: 4.3937435150   0.0959 sec/batch\n",
      "[Validation] Epoch: 11/20   Validation loss: 4.3855609674   0.4016 sec\n",
      "[Train] Epoch: 12/20   Batch: 100/781 (12.80%)   Train loss: 4.3512496948   0.6918 sec/batch\n",
      "[Train] Epoch: 12/20   Batch: 200/781 (25.61%)   Train loss: 4.3579578400   0.3484 sec/batch\n",
      "[Train] Epoch: 12/20   Batch: 300/781 (38.41%)   Train loss: 4.3851375580   0.2339 sec/batch\n",
      "[Train] Epoch: 12/20   Batch: 400/781 (51.22%)   Train loss: 4.4495463371   0.1768 sec/batch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Train] Epoch: 12/20   Batch: 500/781 (64.02%)   Train loss: 4.3661437035   0.1426 sec/batch\n",
      "[Train] Epoch: 12/20   Batch: 600/781 (76.82%)   Train loss: 4.4077596664   0.1197 sec/batch\n",
      "[Train] Epoch: 12/20   Batch: 700/781 (89.63%)   Train loss: 4.3776950836   0.1033 sec/batch\n",
      "[Validation] Epoch: 12/20   Validation loss: 4.3800078685   0.7495 sec\n",
      "[Train] Epoch: 13/20   Batch: 100/781 (12.80%)   Train loss: 4.5186204910   0.7513 sec/batch\n",
      "[Train] Epoch: 13/20   Batch: 200/781 (25.61%)   Train loss: 4.4384679794   0.3786 sec/batch\n",
      "[Train] Epoch: 13/20   Batch: 300/781 (38.41%)   Train loss: 4.2769827843   0.2550 sec/batch\n",
      "[Train] Epoch: 13/20   Batch: 400/781 (51.22%)   Train loss: 4.3247418404   0.1932 sec/batch\n",
      "[Train] Epoch: 13/20   Batch: 500/781 (64.02%)   Train loss: 4.3625121117   0.1555 sec/batch\n",
      "[Train] Epoch: 13/20   Batch: 600/781 (76.82%)   Train loss: 4.3220934868   0.1304 sec/batch\n",
      "[Train] Epoch: 13/20   Batch: 700/781 (89.63%)   Train loss: 4.3627882004   0.1125 sec/batch\n",
      "[Validation] Epoch: 13/20   Validation loss: 4.3828000680   0.4149 sec\n",
      "[Train] Epoch: 14/20   Batch: 100/781 (12.80%)   Train loss: 4.4091906548   0.8099 sec/batch\n",
      "[Train] Epoch: 14/20   Batch: 200/781 (25.61%)   Train loss: 4.4519701004   0.4082 sec/batch\n",
      "[Train] Epoch: 14/20   Batch: 300/781 (38.41%)   Train loss: 4.3253579140   0.2754 sec/batch\n",
      "[Train] Epoch: 14/20   Batch: 400/781 (51.22%)   Train loss: 4.3147764206   0.2094 sec/batch\n",
      "[Train] Epoch: 14/20   Batch: 500/781 (64.02%)   Train loss: 4.4015049934   0.1691 sec/batch\n",
      "[Train] Epoch: 14/20   Batch: 600/781 (76.82%)   Train loss: 4.3592414856   0.1421 sec/batch\n",
      "[Train] Epoch: 14/20   Batch: 700/781 (89.63%)   Train loss: 4.3667497635   0.1226 sec/batch\n",
      "[Validation] Epoch: 14/20   Validation loss: 4.3740114701   0.4180 sec\n",
      "[Train] Epoch: 15/20   Batch: 100/781 (12.80%)   Train loss: 4.3044404984   0.8827 sec/batch\n",
      "[Train] Epoch: 15/20   Batch: 200/781 (25.61%)   Train loss: 4.4169130325   0.4449 sec/batch\n",
      "[Train] Epoch: 15/20   Batch: 300/781 (38.41%)   Train loss: 4.3899345398   0.2990 sec/batch\n",
      "[Train] Epoch: 15/20   Batch: 400/781 (51.22%)   Train loss: 4.3322849274   0.2271 sec/batch\n",
      "[Train] Epoch: 15/20   Batch: 500/781 (64.02%)   Train loss: 4.3741469383   0.1840 sec/batch\n",
      "[Train] Epoch: 15/20   Batch: 600/781 (76.82%)   Train loss: 4.3474993706   0.1552 sec/batch\n",
      "[Train] Epoch: 15/20   Batch: 700/781 (89.63%)   Train loss: 4.2665076256   0.1343 sec/batch\n",
      "[Validation] Epoch: 15/20   Validation loss: 4.3780424338   1.0956 sec\n",
      "[Train] Epoch: 16/20   Batch: 100/781 (12.80%)   Train loss: 4.3781032562   0.9906 sec/batch\n",
      "[Train] Epoch: 16/20   Batch: 200/781 (25.61%)   Train loss: 4.3797397614   0.5013 sec/batch\n",
      "[Train] Epoch: 16/20   Batch: 300/781 (38.41%)   Train loss: 4.3908934593   0.3386 sec/batch\n",
      "[Train] Epoch: 16/20   Batch: 400/781 (51.22%)   Train loss: 4.3649926186   0.2574 sec/batch\n",
      "[Train] Epoch: 16/20   Batch: 500/781 (64.02%)   Train loss: 4.3284215927   0.2088 sec/batch\n",
      "[Train] Epoch: 16/20   Batch: 600/781 (76.82%)   Train loss: 4.3255825043   0.1763 sec/batch\n",
      "[Train] Epoch: 16/20   Batch: 700/781 (89.63%)   Train loss: 4.4358181953   0.1530 sec/batch\n",
      "[Validation] Epoch: 16/20   Validation loss: 4.3733531096   1.2800 sec\n",
      "[Train] Epoch: 17/20   Batch: 100/781 (12.80%)   Train loss: 4.3142881393   1.1206 sec/batch\n",
      "[Train] Epoch: 17/20   Batch: 200/781 (25.61%)   Train loss: 4.3889436722   0.5627 sec/batch\n",
      "[Train] Epoch: 17/20   Batch: 300/781 (38.41%)   Train loss: 4.3697824478   0.3770 sec/batch\n",
      "[Train] Epoch: 17/20   Batch: 400/781 (51.22%)   Train loss: 4.3764963150   0.2846 sec/batch\n",
      "[Train] Epoch: 17/20   Batch: 500/781 (64.02%)   Train loss: 4.4556779861   0.2290 sec/batch\n",
      "[Train] Epoch: 17/20   Batch: 600/781 (76.82%)   Train loss: 4.3718986511   0.1921 sec/batch\n",
      "[Train] Epoch: 17/20   Batch: 700/781 (89.63%)   Train loss: 4.3572406769   0.1655 sec/batch\n",
      "[Validation] Epoch: 17/20   Validation loss: 4.3730106476   0.5450 sec\n",
      "[Train] Epoch: 18/20   Batch: 100/781 (12.80%)   Train loss: 4.4347715378   1.1827 sec/batch\n",
      "[Train] Epoch: 18/20   Batch: 200/781 (25.61%)   Train loss: 4.3488512039   0.5959 sec/batch\n",
      "[Train] Epoch: 18/20   Batch: 300/781 (38.41%)   Train loss: 4.4240040779   0.4012 sec/batch\n",
      "[Train] Epoch: 18/20   Batch: 400/781 (51.22%)   Train loss: 4.3791246414   0.3034 sec/batch\n",
      "[Train] Epoch: 18/20   Batch: 500/781 (64.02%)   Train loss: 4.3124971390   0.2452 sec/batch\n",
      "[Train] Epoch: 18/20   Batch: 600/781 (76.82%)   Train loss: 4.2937569618   0.2057 sec/batch\n",
      "[Train] Epoch: 18/20   Batch: 700/781 (89.63%)   Train loss: 4.3485498428   0.1770 sec/batch\n",
      "[Validation] Epoch: 18/20   Validation loss: 4.3736634646   0.4474 sec\n",
      "[Train] Epoch: 19/20   Batch: 100/781 (12.80%)   Train loss: 4.3764672279   1.2640 sec/batch\n",
      "[Train] Epoch: 19/20   Batch: 200/781 (25.61%)   Train loss: 4.2730321884   0.6358 sec/batch\n",
      "[Train] Epoch: 19/20   Batch: 300/781 (38.41%)   Train loss: 4.4156270027   0.4255 sec/batch\n",
      "[Train] Epoch: 19/20   Batch: 400/781 (51.22%)   Train loss: 4.3988113403   0.3204 sec/batch\n",
      "[Train] Epoch: 19/20   Batch: 500/781 (64.02%)   Train loss: 4.3351607323   0.2573 sec/batch\n",
      "[Train] Epoch: 19/20   Batch: 600/781 (76.82%)   Train loss: 4.3544659615   0.2161 sec/batch\n",
      "[Train] Epoch: 19/20   Batch: 700/781 (89.63%)   Train loss: 4.3757352829   0.1860 sec/batch\n",
      "[Validation] Epoch: 19/20   Validation loss: 4.3712663039   0.4666 sec\n",
      "[Train] Epoch: 20/20   Batch: 100/781 (12.80%)   Train loss: 4.3343448639   1.3263 sec/batch\n",
      "[Train] Epoch: 20/20   Batch: 200/781 (25.61%)   Train loss: 4.3307027817   0.6668 sec/batch\n",
      "[Train] Epoch: 20/20   Batch: 300/781 (38.41%)   Train loss: 4.3745980263   0.4467 sec/batch\n",
      "[Train] Epoch: 20/20   Batch: 400/781 (51.22%)   Train loss: 4.3917984962   0.3365 sec/batch\n",
      "[Train] Epoch: 20/20   Batch: 500/781 (64.02%)   Train loss: 4.3850011826   0.2705 sec/batch\n",
      "[Train] Epoch: 20/20   Batch: 600/781 (76.82%)   Train loss: 4.3774137497   0.2262 sec/batch\n",
      "[Train] Epoch: 20/20   Batch: 700/781 (89.63%)   Train loss: 4.3173575401   0.1948 sec/batch\n",
      "[Validation] Epoch: 20/20   Validation loss: 4.3727535859   0.4189 sec\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "epochs = 20\n",
    "batch_size = 1024\n",
    "print_every_n = 100\n",
    "\n",
    "# Cretae session\n",
    "sess = tf.Session()\n",
    "\n",
    "# Aseguremonos de ver el modelo del grafo en TensorBoard\n",
    "merged = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter('./tmp/run1', sess.graph)\n",
    "\n",
    "# Initialize session variables\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Iterate over opechs\n",
    "for e in range(epochs):\n",
    "    # Iterate over all batches\n",
    "    for users, items, scores, batch_number, total_batches in get_batch(u_train, v_train, r_train, batch_size=batch_size):\n",
    "        train_feed = {\n",
    "            user_input: users,\n",
    "            item_input: items,\n",
    "            y: scores\n",
    "        }\n",
    "\n",
    "        # Feed the graph\n",
    "        batch_loss, _ = sess.run([loss, optimizer], feed_dict=train_feed)\n",
    "            \n",
    "        # Print progress\n",
    "        if (batch_number % print_every_n == 0):\n",
    "            end = time.time()\n",
    "            print('[Train] Epoch: {}/{}  '.format(e+1, epochs),\n",
    "                  'Batch: {}/{} ({:.2f}%)  '.format(batch_number, total_batches, batch_number / total_batches * 100),\n",
    "                  'Train loss: {:.10f}  '.format(batch_loss),\n",
    "                  '{:.4f} sec/batch'.format((end - start) / batch_number))\n",
    "            \n",
    "            sum1 = sess.run(pred_histogram, feed_dict=train_feed)\n",
    "            sum2 = sess.run(loss_histogram, feed_dict=train_feed)  \n",
    "            writer.add_summary(sum1, e)\n",
    "            writer.add_summary(sum2, e)\n",
    "\n",
    "    # Validate with test set\n",
    "    val_start = time.time()\n",
    "    val_loss = 0.\n",
    "    for users, items, scores, batch_number, total_batches in get_batch(u_test, v_test, r_test, batch_size=batch_size):\n",
    "        validation_feed = {\n",
    "            user_input: users,\n",
    "            item_input: items,\n",
    "            y: scores\n",
    "        }\n",
    "        # Feed the graph\n",
    "        val_loss += sess.run([loss], feed_dict=validation_feed)[0]\n",
    "\n",
    "    val_loss /= total_batches\n",
    "    end = time.time()\n",
    "    print('[Validation] Epoch: {}/{}  '.format(e+1, epochs),\n",
    "          'Validation loss: {:.10f}  '.format(val_loss),\n",
    "          '{:.4f} sec'.format((end - val_start)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba del modelo\n",
    "\n",
    "Finalmente, podemos alimentar el grafo con un nuevo diccionario para generar predicciones. En este caso se utiliza el usuario con ID 0 y los productos 1,2 y 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.824893  3.9052625 3.0218327]\n"
     ]
    }
   ],
   "source": [
    "test_dict = {\n",
    "    user_input: [0, 0, 0],\n",
    "    item_input: [1, 2, 3],\n",
    "}\n",
    "predictions = sess.run([y_hat], feed_dict=test_dict)[0]\n",
    "    \n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recsys-class",
   "language": "python",
   "name": "recsys-class"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
