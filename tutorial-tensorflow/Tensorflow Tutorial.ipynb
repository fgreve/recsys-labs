{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial Tensorflow\n",
    "\n",
    "En este tutorial se implementará el modelo de [factorización matricial](http://base.sjtu.edu.cn/~bjshen/2.pdf) con la librería [tensorflow](https://www.tensorflow.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instalación de dependencias\n",
    "```\n",
    "$ pip install numpy\n",
    "$ pip install pandas\n",
    "$ pip install tensorflow\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo primero de todo es importar las librerías que se utilizarán en este tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import time\n",
    "! mkdir tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head ../assignment-2/dataset/ratings.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importación de datos\n",
    "\n",
    "En este tutorial se utilizará el mismo set de datos que el laboratorio práctico Nº2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "data_path = '../assignment-2/dataset/ratings.dat'\n",
    "headers = ['user_id', 'movie_id', 'rating', 'timestamp']\n",
    "data = pd.read_csv(data_path,\n",
    "                   names=headers,\n",
    "                   delimiter=';',\n",
    "                   usecols=['user_id', 'movie_id', 'rating'])\n",
    "\n",
    "print('# Users:', data['user_id'].nunique())\n",
    "print('# Items:', data['movie_id'].nunique())\n",
    "print('# Data:', data.shape[0])\n",
    "data[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez cargado los datos transformamos los ID de usuario y películas mediante un diccionario para que tengan orden correlativo desde 0 a N.\n",
    "\n",
    "Luego separamos el set de entrenamiento y de test, dejando el 20% de los datos para este último.\n",
    "\n",
    "También implementamos una función que nos entregará los datos agrupados en batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prepate batches\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "# Transform data to int\n",
    "user_to_int = {user: i for i, user in enumerate(data['user_id'].unique())}\n",
    "item_to_int = {item: i for i, item in enumerate(data['movie_id'].unique())}\n",
    "\n",
    "# Map user and items to int\n",
    "user_data = data['user_id'].map(lambda user: user_to_int[user]).tolist()\n",
    "item_data = data['movie_id'].map(lambda item: item_to_int[item]).tolist()\n",
    "rating_data = data['rating'].tolist()\n",
    "\n",
    "# split into train / test\n",
    "u_train, u_test, v_train, v_test, r_train, r_test = train_test_split(\n",
    "    user_data, item_data, rating_data, test_size=0.2)\n",
    "\n",
    "def get_batch(user_data, item_data, rating_data, batch_size=32):\n",
    "    # Generate complete batches\n",
    "    count = 0\n",
    "    max_len = len(rating_data)\n",
    "    n_batches = max_len // batch_size\n",
    "    \n",
    "    # Shuffle data\n",
    "    user_data, item_data, rating_data = shuffle(user_data, item_data, rating_data)\n",
    "    \n",
    "    user_data = user_data[0:n_batches*batch_size]\n",
    "    item_data = item_data[0:n_batches*batch_size]\n",
    "    rating_data = rating_data[0:n_batches*batch_size]\n",
    "    \n",
    "    for i in range(0, max_len, batch_size):\n",
    "        count += 1\n",
    "        u = user_data[i:i+batch_size]\n",
    "        v = item_data[i:i+batch_size]\n",
    "        y = rating_data[i:i+batch_size]\n",
    "            \n",
    "        yield u, v, y, count, n_batches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definición del grafo de cómputo\n",
    "\n",
    "En la siguiente celda se utiliza el framework para generar el grafo de cómputo\n",
    "\n",
    "- El primer paso define los parámetros del grafo los cuales corresponden a la cantidad de usuarios e items del set de datos, como también el hiperparámetro k, alpha y el _learning rate_\n",
    "- Luego se definen las variables que serán alimentadas desde el diccionario.\n",
    "- Se inicializan las matrices P y Q del modelo con una distribución uniforme entre [-1, 1]\n",
    "- Los _embeddings_ de usuarios e item son obtenidos mediante una tabla _lookup_\n",
    "- *y_hat* corresponde a la predicción del modelo, i.e. al producto vectorial entre los embeddings\n",
    "- Finalmente definimos la pérdida que se desea optimizar y se crea el objeto optimizador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the graph model\n",
    "\n",
    "# 1. Define model parameters\n",
    "n_users = data['user_id'].nunique()\n",
    "n_items = data['movie_id'].nunique()\n",
    "k = 40\n",
    "alpha = tf.constant(.001, name='alpha')\n",
    "learning_rate = .01\n",
    "\n",
    "# 2. Define variables that are fed through the dictionary session\n",
    "# User, item and ratings placeholders\n",
    "user_input = tf.placeholder(tf.int32, [None], name='user_input')\n",
    "item_input = tf.placeholder(tf.int32, [None], name='item_input')\n",
    "y = tf.placeholder(tf.float32, [None], name='ratings_input')\n",
    "\n",
    "# 3. Define and Initilize matrix embeddings\n",
    "# User embeddings come from P matrix\n",
    "# Item embeddings come from Q matrix\n",
    "\n",
    "with tf.name_scope('P_matrix'):\n",
    "    P_matrix = tf.Variable(tf.random_uniform((n_users, k), -1, 1), name='user_embeddings')\n",
    "    tf.summary.tensor_summary('P_matrix', P_matrix)\n",
    "with tf.name_scope('Q_matrix'):\n",
    "    Q_matrix = tf.Variable(tf.random_uniform((n_items, k), -1, 1), name='item_embeddings')\n",
    "    tf.summary.tensor_summary('Q_matrix', Q_matrix)\n",
    "\n",
    "# 4. Fetch embeddings with a lookup table\n",
    "# Define user and item embedding\n",
    "user_embed = tf.nn.embedding_lookup(P_matrix, user_input, name='user_embed')\n",
    "item_embed = tf.nn.embedding_lookup(Q_matrix, item_input, name='item_embed')\n",
    "\n",
    "# 5. Compute prediction\n",
    "with tf.name_scope('prediction'):\n",
    "    y_hat = tf.reduce_sum(tf.multiply(user_embed, item_embed), 1)\n",
    "    tf.summary.scalar('prediction', y_hat)\n",
    "    pred_histogram = tf.summary.histogram(\"mean_prediction\", y_hat)\n",
    "\n",
    "    \n",
    "# Compute loss function\n",
    "# loss = 1/n (y - y_hat) ** 2\n",
    "mse_loss = tf.losses.mean_squared_error(y, y_hat)\n",
    "\n",
    "# reg_loss = alpha * (||p|| + ||q||)\n",
    "reg_loss = tf.add(tf.multiply(alpha, tf.nn.l2_loss(user_embed)), tf.multiply(alpha, tf.nn.l2_loss(item_embed)))\n",
    "\n",
    "with tf.name_scope('error'):\n",
    "    loss = tf.add(mse_loss, reg_loss)\n",
    "    tf.summary.scalar('error', loss)\n",
    "    loss_histogram = tf.summary.histogram(\"mean_loss\", loss)\n",
    "\n",
    "    \n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento del modelo\n",
    "\n",
    "El primer paso para poder ejecutar el modelo ya definido es generar un objeto de sesión e inicializar las variables. Luego iteramos las veces que se quiere para ajustar el modelo.\n",
    "\n",
    "Para el ajuste del modelo, se genera un diccionario que alimenta el grafo de cómputo y luego se evalúa la sesión con el diccionario. En este caso se evalúa la pérdida y el objeto optimizador, el cual es el encargado de calcular el gradiente y actualizar los parámetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train model\n",
    "epochs = 20\n",
    "batch_size = 1024\n",
    "print_every_n = 100\n",
    "\n",
    "# Cretae session\n",
    "sess = tf.Session()\n",
    "\n",
    "# Aseguremonos de ver el modelo del grafo en TensorBoard\n",
    "merged = tf.summary.merge_all()\n",
    "writer = tf.summary.FileWriter('./tmp/run1', sess.graph)\n",
    "\n",
    "# Initialize session variables\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# Iterate over opechs\n",
    "for e in range(epochs):\n",
    "    # Iterate over all batches\n",
    "    for users, items, scores, batch_number, total_batches in get_batch(u_train, v_train, r_train, batch_size=batch_size):\n",
    "        train_feed = {\n",
    "            user_input: users,\n",
    "            item_input: items,\n",
    "            y: scores\n",
    "        }\n",
    "\n",
    "        # Feed the graph\n",
    "        batch_loss, _ = sess.run([loss, optimizer], feed_dict=train_feed)\n",
    "            \n",
    "        # Print progress\n",
    "        if (batch_number % print_every_n == 0):\n",
    "            end = time.time()\n",
    "            print('[Train] Epoch: {}/{}  '.format(e+1, epochs),\n",
    "                  'Batch: {}/{} ({:.2f}%)  '.format(batch_number, total_batches, batch_number / total_batches * 100),\n",
    "                  'Train loss: {:.10f}  '.format(batch_loss),\n",
    "                  '{:.4f} sec/batch'.format((end - start) / batch_number))\n",
    "            \n",
    "            sum1 = sess.run(pred_histogram, feed_dict=train_feed)\n",
    "            sum2 = sess.run(loss_histogram, feed_dict=train_feed)  \n",
    "            writer.add_summary(sum1, e)\n",
    "            writer.add_summary(sum2, e)\n",
    "\n",
    "    # Validate with test set\n",
    "    val_start = time.time()\n",
    "    val_loss = 0.\n",
    "    for users, items, scores, batch_number, total_batches in get_batch(u_test, v_test, r_test, batch_size=batch_size):\n",
    "        validation_feed = {\n",
    "            user_input: users,\n",
    "            item_input: items,\n",
    "            y: scores\n",
    "        }\n",
    "        # Feed the graph\n",
    "        val_loss += sess.run([loss], feed_dict=validation_feed)[0]\n",
    "\n",
    "    val_loss /= total_batches\n",
    "    end = time.time()\n",
    "    print('[Validation] Epoch: {}/{}  '.format(e+1, epochs),\n",
    "          'Validation loss: {:.10f}  '.format(val_loss),\n",
    "          '{:.4f} sec'.format((end - val_start)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prueba del modelo\n",
    "\n",
    "Finalmente, podemos alimentar el grafo con un nuevo diccionario para generar predicciones. En este caso se utiliza el usuario con ID 0 y los productos 1,2 y 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dict = {\n",
    "    user_input: [0, 0, 0],\n",
    "    item_input: [1, 2, 3],\n",
    "}\n",
    "predictions = sess.run([y_hat], feed_dict=test_dict)[0]\n",
    "    \n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_iter = get_batch(u_test, v_test, r_test, batch_size=batch_size)\n",
    "next(batch_iter)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "recsys-class",
   "language": "python",
   "name": "recsys-class"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
